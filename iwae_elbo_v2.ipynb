{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, Layer, TimeDistributed, Concatenate, Lambda\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "from vae_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('./data/iwae/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "def preprocess(data):\n",
    "    x, y = data\n",
    "    x = x.reshape((len(x), 28, 28))\n",
    "    x = x/255.\n",
    "    #y = to_categorical(y, 10) #don't need to categorise y\n",
    "    return x, y\n",
    "\n",
    "train, test = mnist.load_data()\n",
    "\n",
    "x_train, y_train = preprocess(train)\n",
    "x_test, y_test = preprocess(test)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a layer that samples a latent variable given a mean and standard deviation\n",
    "\n",
    "class Sampler(Layer):\n",
    "    \n",
    "    def __init__(self, always_sample=False, **kwargs):\n",
    "        self.stddev = 1\n",
    "        self.always_sample = always_sample\n",
    "        super(Sampler, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, x, training=None):\n",
    "        assert isinstance(x, list)\n",
    "        z_mean, log_z_var = x\n",
    "        z_std = K.exp(log_z_var/2)\n",
    "        \n",
    "        # sample epsilon from N(0, stddev)\n",
    "        shape = K.shape(z_std)\n",
    "        epsilon = K.random_normal(shape, mean=0, stddev=self.stddev)\n",
    "        z_sample = z_mean + z_std * epsilon\n",
    "        \n",
    "        if self.always_sample:\n",
    "            return z_sample\n",
    "        else:\n",
    "            return K.in_train_phase(z_sample, z_mean, training=training)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        assert input_shape[0] == input_shape[1]\n",
    "        return input_shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(k, latent_dim=100, hidden_dim=200, always_sample=False):\n",
    "    # encoder shared layers\n",
    "    enc_hid_1 = Dense(hidden_dim, activation='tanh', name='enc_1_hidden_1')\n",
    "    enc_hid_2 = Dense(hidden_dim, activation='tanh', name='enc_1_hidden_2')\n",
    "    z_mean = Dense(latent_dim, name='enc_1_latent_mean')\n",
    "    log_z_var = Dense(latent_dim, name='enc_1_log_latent_var')\n",
    "    sampler = Sampler(always_sample, name='z1_sampler')\n",
    "    \n",
    "    # decoder shared layers\n",
    "    dec_hid_1 = Dense(hidden_dim, activation='tanh', name='dec_1_hidden_1')\n",
    "    dec_hid_2 = Dense(hidden_dim, activation='tanh', name='dec_1_hidden_2')\n",
    "    bernoulli_mean = Dense(28*28, activation='sigmoid', name='dec_1_mean')\n",
    "    reshape = Reshape((28, 28), name='dec_1_output')\n",
    "    \n",
    "    # single pass model\n",
    "    x = Input(shape=(28, 28), name='enc_1_input')\n",
    "    y = Flatten(name='enc_1_flatten')(x)\n",
    "    y = enc_hid_1(y)\n",
    "    y = enc_hid_2(y)\n",
    "    mu = z_mean(y)\n",
    "    log_var = log_z_var(y)\n",
    "    z1 = sampler([z_mean(y), log_z_var(y)])\n",
    "    y = dec_hid_1(z1)\n",
    "    y = dec_hid_2(y)\n",
    "    y = bernoulli_mean(y)\n",
    "    y = reshape(y)\n",
    "    \n",
    "    model = Model(x, y, name='model_1')\n",
    "    \n",
    "    # k forward passes - start from first sampling layer\n",
    "    k_z1 = [sampler([mu, log_var]) for i in range(k)]\n",
    "    k_y = [dec_hid_1(z1) for z1 in k_z1]\n",
    "    k_y = [dec_hid_2(y) for y in k_y]\n",
    "    k_y = [bernoulli_mean(y) for y in k_y]\n",
    "    k_y = [reshape(y) for y in k_y]\n",
    "    \n",
    "    return model, mu, log_var, k_z1, k_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training k forward pass model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iwae_loss(y_true, y_pred):\n",
    "    # calculate log distributions (log_p_x_y := p(x|y))\n",
    "    elbo = []\n",
    "    weights = []\n",
    "    for i in range(k):\n",
    "        log_q_h1_x = -0.5 * K.sum(log_z_var + K.exp(-log_z_var)*(z1_samples[i] - z_mean)**2, axis=-1)\n",
    "        log_p_h1 = -0.5 * K.sum(K.square(z1_samples[i]), axis=-1)\n",
    "        log_p_x_h1 = -K.sum(K.binary_crossentropy(y_true, out_samples[i]), axis=(-1, -2))\n",
    "\n",
    "        # calculate weights\n",
    "        log_weights = log_p_x_h1 + log_p_h1 - log_q_h1_x\n",
    "        weights.append(log_weights)\n",
    "        \n",
    "        elbo.append(log_p_x_h1 + log_p_h1 - log_q_h1_x)\n",
    "\n",
    "    weights = K.softmax(weights, axis=0)\n",
    "    elbo = K.sum(weights * elbo, axis=0)\n",
    "\n",
    "    loss = -elbo\n",
    "\n",
    "    return loss\n",
    "\n",
    "def vae_loss(y_true, y_pred):\n",
    "    loss = 0\n",
    "    for i in range(k):\n",
    "        log_q_h1_x = -0.5 * K.sum(log_z_var + K.exp(-log_z_var)*(z1_samples[i] - z_mean)**2, axis=-1)\n",
    "        log_p_h1 = -0.5 * K.sum(K.square(z1_samples[i]), axis=-1)\n",
    "        log_p_x_h1 = -K.sum(K.binary_crossentropy(y_true, out_samples[i]), axis=(-1, -2))\n",
    "\n",
    "        elbo = log_p_x_h1 + log_p_h1 - log_q_h1_x\n",
    "        loss -= elbo\n",
    "\n",
    "    return loss/k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_1(k, latent_dim=100, epochs=50, batch_size=512, train='both'):\n",
    "    out = []\n",
    "    hists = []\n",
    "    \n",
    "    if train.lower() == 'iwae' or train.lower() == 'both':\n",
    "        # IWAE training\n",
    "        print('Training IWAE model')\n",
    "        print('-------------------')\n",
    "\n",
    "        iwae_model, z_mean, log_z_var, z1_samples, out_samples = model_1(k, latent_dim)\n",
    "        \n",
    "        def iwae_loss(y_true, y_pred):\n",
    "            # calculate log distributions (log_p_x_y := p(x|y))\n",
    "            log_weights = []\n",
    "            for i in range(k):\n",
    "                log_q_h1_x = -0.5 * K.sum(log_z_var + K.exp(-log_z_var)*(z1_samples[i] - z_mean)**2, axis=-1)\n",
    "                log_p_h1 = -0.5 * K.sum(K.square(z1_samples[i]), axis=-1)\n",
    "                log_p_x_h1 = -K.sum(K.binary_crossentropy(y_true, out_samples[i]), axis=(-1, -2))\n",
    "\n",
    "                # calculate weights\n",
    "                log_weight = log_p_x_h1 + log_p_h1 - log_q_h1_x\n",
    "                log_weights.append(log_weight)\n",
    "                \n",
    "            weights = K.exp(log_weights - K.max(log_weights, axis=0, keepdims=True))\n",
    "            elbo = 1/k * K.sum(weights, axis=0)\n",
    "            elbo = K.log(elbo) + K.max(log_weights, axis=0, keepdims=True)\n",
    "            \n",
    "            # let's try adding the second order term and see what happens\n",
    "            v_sqr = K.square(log_weights)\n",
    "            v_sqr = 1/k * K.sum(v_sqr, axis=0)\n",
    "            second_ord = -1/2 * (v_sqr - elbo**2)\n",
    "            elbo = elbo + second_ord\n",
    "            \n",
    "            loss = -elbo\n",
    "\n",
    "            return loss\n",
    "\n",
    "        iwae_model.compile(optimizer='adam', loss=iwae_loss)\n",
    "        hist = iwae_model.fit(x_train, x_train, batch_size=batch_size, epochs=epochs)\n",
    "        \n",
    "        out.append(iwae_model)\n",
    "        hists.append(hist)\n",
    "\n",
    "        model_path = './iwae_model_1_k_%d_dim_%d.weights' %(k, latent_dim)\n",
    "        iwae_model.save_weights(model_path)\n",
    "        \n",
    "        \n",
    "    if train.lower() == 'vae' or train.lower() == 'both':\n",
    "        # VAE training\n",
    "        if train.lower() == 'both':\n",
    "            print('\\n')\n",
    "        print('Training VAE model')\n",
    "        print('------------------')\n",
    "        \n",
    "        vae_model, z_mean, log_z_var, z1_samples, out_samples = model_1(k, latent_dim)\n",
    "        \n",
    "        def vae_loss(y_true, y_pred):\n",
    "            loss = 0\n",
    "            elbos = []\n",
    "            for i in range(k):\n",
    "                log_q_h1_x = -0.5 * K.sum(log_z_var + K.exp(-log_z_var)*(z1_samples[i] - z_mean)**2, axis=-1)\n",
    "                log_p_h1 = -0.5 * K.sum(K.square(z1_samples[i]), axis=-1)\n",
    "                log_p_x_h1 = -K.sum(K.binary_crossentropy(y_true, out_samples[i]), axis=(-1, -2))\n",
    "\n",
    "                elbo = log_p_x_h1 + log_p_h1 - log_q_h1_x\n",
    "                elbos.append(elbo)\n",
    "                loss -= elbo\n",
    "            \n",
    "            loss = loss / k\n",
    "            \n",
    "            # uncomment to see what happens when we add the second order term to the vae loss\n",
    "            #elbo = 1/k * K.sum(elbos, axis=0)\n",
    "            #v_sqr = K.square(elbos)\n",
    "            #v_sqr = 1/k * K.sum(v_sqr, axis=0)\n",
    "            #second_ord = -1/2 * (v_sqr - elbo**2)\n",
    "            #elbo = elbo + second_ord\n",
    "            \n",
    "            #loss = -elbo\n",
    "            \n",
    "            return loss\n",
    "\n",
    "        vae_model.compile(optimizer='adam', loss=vae_loss)\n",
    "        hist = vae_model.fit(x_train, x_train, batch_size=batch_size, epochs=epochs)\n",
    "        \n",
    "        out.append(vae_model)\n",
    "        hists.append(hist)\n",
    "\n",
    "        model_path = './vae_model_1_k_%d_dim_%d.weights' %(k, latent_dim)\n",
    "        vae_model.save_weights(model_path)\n",
    "    \n",
    "    \n",
    "    if train.lower() not in ['both', 'vae', 'iwae']:\n",
    "        print('Set parameter train to \"both\", \"vae\", or \"iwae\".')\n",
    "        return\n",
    "    \n",
    "    if train.lower() == 'both':\n",
    "        return out, hists\n",
    "    return out[0], hists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch trainer\n",
    "\n",
    "def train_for_k(ks, latent_dim=100, train='both'):\n",
    "    ks = np.asarray(ks)\n",
    "    print('TRAINING')\n",
    "    for k in ks:\n",
    "        print('\\n')\n",
    "        print('-------------------')\n",
    "        print('k = %d' %k)\n",
    "        print('latent_dim = %d' %latent_dim)\n",
    "        print('-------------------\\n')\n",
    "        train_model_1(k, latent_dim, train=train)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ks = [1, 5, 10, 20, 30, 40, 50]\n",
    "ks = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "\n",
      "\n",
      "-------------------\n",
      "k = 10\n",
      "latent_dim = 100\n",
      "-------------------\n",
      "\n",
      "Training VAE model\n",
      "------------------\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 20s 341us/step - loss: 273.4655\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 14s 241us/step - loss: 224.6009\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 221.7762\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 15s 247us/step - loss: 228.5148\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 15s 246us/step - loss: 218.0927\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 15s 248us/step - loss: 229.5561\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 15s 249us/step - loss: 226.1526\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 15s 251us/step - loss: 215.8805\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 25s 412us/step - loss: 223.5479\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 27s 452us/step - loss: 215.5045\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 27s 451us/step - loss: 213.9297\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 27s 452us/step - loss: 223.0881\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 27s 452us/step - loss: 215.0186\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 27s 452us/step - loss: 218.1546\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 27s 454us/step - loss: 218.0482\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 27s 454us/step - loss: 222.4399\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 27s 454us/step - loss: 219.8517\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 27s 456us/step - loss: 213.3757\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 27s 453us/step - loss: 215.4190\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 27s 454us/step - loss: 216.1198\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 27s 454us/step - loss: 209.6406\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 205.8081\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 202.2454\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 27s 452us/step - loss: 200.2794\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 27s 452us/step - loss: 211.2168\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 199.6304\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 27s 452us/step - loss: 234.4957\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 27s 452us/step - loss: 208.6000\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 27s 452us/step - loss: 249.1017\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 27s 454us/step - loss: 219.7207\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 196.4623\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 195.7651\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 27s 456us/step - loss: 211.3544\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 194.1888\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 224.9660\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 50s 840us/step - loss: 192.1348\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 52s 875us/step - loss: 238.0984\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 27s 452us/step - loss: 197.7285\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 27s 453us/step - loss: 194.2392\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 27s 453us/step - loss: 221.8502\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 27s 453us/step - loss: 195.6556\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 27s 454us/step - loss: 221.4013\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 203.2467\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 33s 556us/step - loss: 190.8003\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 57s 942us/step - loss: 188.2309\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 37s 612us/step - loss: 188.7360\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 187.3440\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 42s 692us/step - loss: 189.3332\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 57s 944us/step - loss: 187.9380\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 57s 944us/step - loss: 217.7780\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_for_k(ks, train='vae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IWAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "\n",
    "load_k = 10\n",
    "latent_dim = 100\n",
    "\n",
    "iwae_path = 'iwae_model_1_k_%d_dim_%d.weights' %(load_k, latent_dim)\n",
    "    \n",
    "# load model\n",
    "iwae_model, z_mean, log_z_var, z1_samples, out_samples = model_1(k, latent_dim, always_sample=True)\n",
    "iwae_model.load_weights(iwae_path, by_name=True)\n",
    "\n",
    "k_iwae_model = Model(iwae_model.input, out_samples)\n",
    "k_z_samples = Model(iwae_model.input, z1_samples)\n",
    "enc_model = Model(iwae_model.input, [z_mean, log_z_var])\n",
    "\n",
    "iwae_model.compile(optimizer='adam', loss=iwae_loss)\n",
    "\n",
    "x_preds = k_iwae_model.predict(x_test, batch_size=512)\n",
    "z1_samples = k_z_samples.predict(x_test, batch_size=512)\n",
    "z_mean, log_z_var = enc_model.predict(x_test, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iwae_elbo(y_true, y_pred):\n",
    "    # calculate log distributions (log_p_x_y := p(x|y))\n",
    "    elbo = []\n",
    "    log_weights = []\n",
    "    for i in range(k):\n",
    "        log_q_h1_x = -0.5 * np.sum(log_z_var + np.exp(-log_z_var)*(z1_samples[i] - z_mean)**2, axis=-1)\n",
    "        log_p_h1 = -0.5 * np.sum(z1_samples[i]**2, axis=-1)\n",
    "        log_p_x_h1 = np.sum(y_true * np.log(y_pred[i]) + (1 - y_true) * np.log(1 - y_pred[i]), axis=(-1, -2))\n",
    "\n",
    "        # calculate weights\n",
    "        log_weight = log_p_x_h1 + log_p_h1 - log_q_h1_x\n",
    "        log_weights.append(log_weight)\n",
    "    \n",
    "    #weights = np.exp(log_weights - np.max(log_weights, axis=1, keepdims=True))\n",
    "    #weights = weights / sum(weights, axis=0)\n",
    "    var = np.var(log_weights, axis=0)\n",
    "    #elbo = np.sum(weights * log_weights, axis=0)\n",
    "    \n",
    "    weights = np.exp(log_weights - np.max(log_weights, axis=0, keepdims=True))\n",
    "    elbo = 1/k * np.sum(weights, axis=0)\n",
    "    elbo = np.log(elbo) + np.max(log_weights, axis=0, keepdims=True)\n",
    "    loss = -elbo\n",
    "\n",
    "    return elbo, var, log_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo, var, log_weights = iwae_elbo(x_test, x_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ -72.37829892, -109.90690892,  -40.300041  , ...,  -77.9211534 ,\n",
       "         -101.36395393, -123.6241121 ]]),\n",
       " -93.68109201047788,\n",
       " array([132.75004576, 131.25136892, 140.8115718 , ...,  74.33978318,\n",
       "        162.48521614, 129.71405675]),\n",
       " 124.346180775778)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elbo, mean(elbo), var, mean(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ -72.10513569, -112.07000651,  -37.50728277, ...,  -80.86118173,\n",
       "          -96.71502434, -124.02455561]]),\n",
       " -93.56910197242789,\n",
       " array([ 66.0260446 ,  99.83693518,  80.51568945, ...,  49.71792848,\n",
       "        115.92595999,  94.69907044]),\n",
       " 73.40518760855443)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elbo, mean(elbo), var, mean(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_u = np.var(z_mean, axis=0)\n",
    "sum(log(A_u) >= -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "\n",
    "load_k = 10\n",
    "latent_dim = 100\n",
    "\n",
    "iwae_path = 'vae_model_1_k_%d_dim_%d.weights' %(load_k, latent_dim)\n",
    "    \n",
    "# load model\n",
    "iwae_model, z_mean, log_z_var, z1_samples, out_samples = model_1(k, latent_dim, always_sample=True)\n",
    "iwae_model.load_weights(iwae_path, by_name=True)\n",
    "\n",
    "k_iwae_model = Model(iwae_model.input, out_samples)\n",
    "k_z_samples = Model(iwae_model.input, z1_samples)\n",
    "enc_model = Model(iwae_model.input, [z_mean, log_z_var])\n",
    "\n",
    "iwae_model.compile(optimizer='adam', loss=iwae_loss)\n",
    "\n",
    "x_preds = k_iwae_model.predict(x_test, batch_size=512)\n",
    "z1_samples = k_z_samples.predict(x_test, batch_size=512)\n",
    "z_mean, log_z_var = enc_model.predict(x_test, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_elbo(y_true, y_pred):\n",
    "    # calculate log distributions (log_p_x_y := p(x|y))\n",
    "    elbos = []\n",
    "    for i in range(k):\n",
    "        log_q_h1_x = -0.5 * np.sum(log_z_var + np.exp(-log_z_var)*(z1_samples[i] - z_mean)**2, axis=-1)\n",
    "        log_p_h1 = -0.5 * np.sum(z1_samples[i]**2, axis=-1)\n",
    "        log_p_x_h1 = np.sum(y_true * np.log(y_pred[i]) + (1 - y_true) * np.log(1 - y_pred[i]), axis=(-1, -2))\n",
    "        \n",
    "        elbos.append(log_p_x_h1 + log_p_h1 - log_q_h1_x)\n",
    "    \n",
    "    var = np.var(elbos, axis=0)\n",
    "    elbo = np.mean(elbos, axis=0)\n",
    "\n",
    "    return elbo, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo, var = vae_elbo(x_test, x_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-165.21271034, -252.591892  ,  -88.20299404, ..., -165.88884886,\n",
       "        -226.65836364, -245.90310142]),\n",
       " -196.51514868754143,\n",
       " array([4.28513664, 6.51203712, 4.99395905, ..., 4.20919283, 3.47501041,\n",
       "        9.21949225]),\n",
       " 5.142778526796924)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elbo, mean(elbo), var, mean(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ -69.19617049, -114.037363  ,  -35.44858195, ...,  -77.55053169,\n",
       "        -104.04649974, -119.47013777]),\n",
       " -93.71715004979777,\n",
       " array([19.18326246, 18.14852099, 17.93589541, ..., 30.9315636 ,\n",
       "        19.10827392, 26.10820491]),\n",
       " 20.589260445661967)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elbo, mean(elbo), var, mean(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_u = np.var(z_mean, axis=0)\n",
    "sum(log(A_u) >= -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
