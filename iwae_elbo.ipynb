{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, Layer, TimeDistributed, Concatenate, Lambda\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "from vae_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('./data/iwae/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "def preprocess(data):\n",
    "    x, y = data\n",
    "    x = x.reshape((len(x), 28, 28))\n",
    "    x = x/255.\n",
    "    #y = to_categorical(y, 10) #don't need to categorise y\n",
    "    return x, y\n",
    "\n",
    "train, test = mnist.load_data()\n",
    "\n",
    "x_train, y_train = preprocess(train)\n",
    "x_test, y_test = preprocess(test)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a layer that samples a latent variable given a mean and standard deviation\n",
    "\n",
    "class Sampler(Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        self.stddev = 1\n",
    "        super(Sampler, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, x, training=None):\n",
    "        assert isinstance(x, list)\n",
    "        z_mean, log_z_var = x\n",
    "        z_std = K.exp(log_z_var/2)\n",
    "        # sample epsilon from N(0, stddev)\n",
    "        shape = K.shape(z_std)\n",
    "        epsilon = K.random_normal(shape, mean=0, stddev=self.stddev)\n",
    "        # we sample during training only\n",
    "        z_sample = z_mean + z_std * epsilon\n",
    "        return K.in_train_phase(z_sample, z_mean, training=training)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        assert input_shape[0] == input_shape[1]\n",
    "        return input_shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_1(k, latent_dim=100, hidden_dim=200):\n",
    "    # shared layers\n",
    "    hid_1 = Dense(hidden_dim, activation='tanh', name='enc_1_hidden_1')\n",
    "    hid_2 = Dense(hidden_dim, activation='tanh', name='enc_1_hidden_2')\n",
    "    z_mean = Dense(latent_dim, name='enc_1_latent_mean')\n",
    "    log_z_var = Dense(latent_dim, name='enc_1_log_latent_var')\n",
    "    \n",
    "    # define time distributed model for training for k forward passes\n",
    "    td_x = Input(shape=(k, 28, 28), name='td_enc_1_input')\n",
    "    y = Reshape((k, 28*28), name='td_enc_1_flatten')(td_x)\n",
    "    y = TimeDistributed(hid_1, name='td_enc_1_hidden_1')(y)\n",
    "    y = TimeDistributed(hid_2, name='td_enc_1_hidden_2')(y)\n",
    "    td_z_mean = TimeDistributed(z_mean, name='td_enc_1_latent_mean')(y)\n",
    "    td_log_z_var = TimeDistributed(log_z_var, name='td_enc_1_log_latent_var')(y)\n",
    "    \n",
    "    td_model = Model(inputs=td_x, outputs=[td_z_mean, td_log_z_var], name='td_encoder_1')\n",
    "    \n",
    "    # define single forward pass model\n",
    "    x = Input(shape=(28, 28), name='enc_1_input')\n",
    "    y = Flatten(name='enc_1_flatten')(x)\n",
    "    y = hid_1(y)\n",
    "    y = hid_2(y)\n",
    "    y = Sampler(name='encoder_1_sampler')([z_mean(y), log_z_var(y)])\n",
    "    \n",
    "    model = Model(inputs=x, outputs=y, name='encoder_1')\n",
    "    td = TimeDistributed(model, name='td_enc_1')(td_x)\n",
    "    td_model = Model(td_x, td, name='td_encoder_1')\n",
    "    \n",
    "    return td_model, model\n",
    "\n",
    "\n",
    "def encoder_2(k, latent_dim=100, hidden_dim=100):\n",
    "    # shared layers\n",
    "    hid_1 = Dense(hidden_dim, activation='tanh', name='enc_2_hidden_1')\n",
    "    hid_2 = Dense(hidden_dim, activation='tanh', name='enc_2_hidden_2')\n",
    "    z_mean = Dense(latent_dim, name='enc_2_latent_mean')\n",
    "    log_z_var = Dense(latent_dim, name='enc_2_log_latent_var')\n",
    "    \n",
    "    # define time distributed model for training for k forward passes\n",
    "    td_x = Input(shape=(k, latent_dim), name='td_enc_2_input')\n",
    "    y = TimeDistributed(hid_1, name='td_enc_2_hidden_1')(td_x)\n",
    "    y = TimeDistributed(hid_2, name='td_enc_2_hidden_2')(y)\n",
    "    td_z_mean = TimeDistributed(z_mean, name='td_enc_2_latent_mean')(y)\n",
    "    td_log_z_var = TimeDistributed(log_z_var, name='td_enc_2_log_latent_var')(y)\n",
    "    \n",
    "    td_model = Model(inputs=td_x, outputs=[td_z_mean, td_log_z_var], name='td_encoder_2')\n",
    "    \n",
    "    # define single forward pass model\n",
    "    x = Input(shape=(latent_dim,), name='enc_2_input')\n",
    "    y = hid_1(x)\n",
    "    y = hid_2(y)\n",
    "    y = Sampler()([z_mean(y), log_z_var(y)])\n",
    "    \n",
    "    model = Model(inputs=x, outputs=y, name='encoder_2')\n",
    "    td = TimeDistributed(model, name='td_enc_2')(td_x)\n",
    "    td_model = Model(td_x, td, name='td_encoder_2')\n",
    "    \n",
    "    return td_model, model\n",
    "\n",
    "\n",
    "def decoder_2(k, latent_dim=100, hidden_dim=100):\n",
    "    # shared layers\n",
    "    hid_1 = Dense(hidden_dim, activation='tanh', name='dec_2_hidden_1')\n",
    "    hid_2 = Dense(hidden_dim, activation='tanh', name='dec_2_hidden_2')\n",
    "    z_mean = Dense(latent_dim, name='dec_2_latent_mean')\n",
    "    log_z_var = Dense(latent_dim, name='dec_2_log_latent_var')\n",
    "    \n",
    "    # define time distributed model for training for k forward passes\n",
    "    td_x = Input(shape=(k, latent_dim), name='td_dec_2_input')\n",
    "    y = TimeDistributed(hid_1, name='td_dec_2_hidden_1')(td_x)\n",
    "    y = TimeDistributed(hid_2, name='td_dec_2_hidden_2')(y)\n",
    "    td_z_mean = TimeDistributed(z_mean, name='td_dec_2_latent_mean')(y)\n",
    "    td_log_z_var = TimeDistributed(log_z_var, name='td_dec_2_log_latent_var')(y)\n",
    "    \n",
    "    td_model = Model(inputs=td_x, outputs=[td_z_mean, td_log_z_var], name='td_decoder_2')\n",
    "    \n",
    "    # define single forward pass model\n",
    "    x = Input(shape=(latent_dim,), name='dec_2_input')\n",
    "    y = hid_1(x)\n",
    "    y = hid_2(y)\n",
    "    y = Sampler()([z_mean(y), log_z_var(y)])\n",
    "    \n",
    "    model = Model(inputs=x, outputs=y, name='decoder_2')\n",
    "    td = TimeDistributed(model)(td_x)\n",
    "    td_model = Model(td_x, td, name='td_decoder_2')\n",
    "    \n",
    "    return td_model, model\n",
    "\n",
    "\n",
    "def decoder_1(k, latent_dim=100, hidden_dim=200):\n",
    "    # shared layers\n",
    "    hid_1 = Dense(hidden_dim, activation='tanh', name='dec_1_hidden_1')\n",
    "    hid_2 = Dense(hidden_dim, activation='tanh', name='dec_1_hidden_2')\n",
    "    bernoulli_mean = Dense(28*28, activation='sigmoid', name='dec_1_mean')\n",
    "    \n",
    "    # define time distributed model for training for k forward passes\n",
    "    td_x = Input(shape=(k, latent_dim), name='td_dec_1_input')\n",
    "    y = TimeDistributed(hid_1, name='td_dec_1_hidden_1')(td_x)\n",
    "    y = TimeDistributed(hid_2, name='td_dec_1_hidden_2')(y)\n",
    "    y = TimeDistributed(bernoulli_mean, name='td_dec_1_mean')(y)\n",
    "    y = TimeDistributed(Reshape((28, 28), name='td_dec_1_output'))(y)\n",
    "    \n",
    "    td_model = Model(inputs=td_x, outputs=y, name='td_decoder_1')\n",
    "    \n",
    "    # define single forward pass model\n",
    "    x = Input(shape=(latent_dim,), name='dec_1_input')\n",
    "    y = hid_1(x)\n",
    "    y = hid_2(y)\n",
    "    y = bernoulli_mean(y)\n",
    "    y = Reshape((28, 28), name='dec_1_output')(y)\n",
    "    \n",
    "    model = Model(inputs=x, outputs=y, name='decoder_1')\n",
    "    td = TimeDistributed(model, name='td_dec_1')(td_x)\n",
    "    td_model = Model(td_x, td, name='td_decoder_1')\n",
    "    \n",
    "    return td_model, model\n",
    "\n",
    "\n",
    "def connect_enc_dec(encoder, decoder, name=None):\n",
    "    if isinstance(encoder, list) and isinstance(decoder, list):\n",
    "        x = Input(batch_shape = encoder[0].input_shape, name=encoder[0].name + '_input')\n",
    "        z_mean, log_z_var = encoder[0](x)\n",
    "        # connect encoders\n",
    "        for i in range(len(encoder)-1):\n",
    "            sampler, _ = Sampler(name=encoder[i].name + '_sampler')([z_mean, log_z_var])\n",
    "            z_mean, log_z_var = encoder[i+1](sampler)\n",
    "        # connect final encoder to decoder\n",
    "        sampler = Sampler(name=encoder[-1].name + '_sampler')([z_mean, log_z_var])\n",
    "        # connect decoders\n",
    "        for i in range(len(decoder)-1):\n",
    "            z_mean, log_z_var = decoder[i](sampler)\n",
    "            sampler, _ = Sampler(name=decoder[i].name + '_sampler')([z_mean, log_z_var])\n",
    "        # get the output\n",
    "        out = decoder[-1](sampler)\n",
    "        return Model(x, out)\n",
    "    else:\n",
    "        x = Input(batch_shape = encoder.input_shape, name=encoder.name + '_input')\n",
    "        z_mean, log_z_var = encoder.outputs\n",
    "        sampler = Sampler(name=encoder.name + '_sampler')([z_mean, log_z_var])\n",
    "        return  Model(encoder.inputs, decoder(sampler), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(k):\n",
    "    td_enc, enc = encoder_1(k)\n",
    "    td_dec, dec = decoder_1(k)\n",
    "    \n",
    "    #td_model = connect_enc_dec(td_enc, td_dec, name='td_model_1')\n",
    "    #model = connect_enc_dec(enc, dec, name='model_1')\n",
    "    td_model = Model(td_enc.inputs, td_dec(td_enc.output))\n",
    "    model = Model(enc.inputs, dec(enc.output))\n",
    "    return [td_model, td_enc, td_dec], [model, enc, dec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(k):\n",
    "    td_enc_1, enc_1 = encoder_1(k)\n",
    "    td_enc_2, enc_2 = encoder_2(k)\n",
    "    td_dec_2, dec_2 = decoder_2(k)\n",
    "    td_dec_1, dec_1 = decoder_1(k)\n",
    "    \n",
    "    td_model = connect_enc_dec([td_enc_1, td_enc_2], [td_dec_2, td_dec_1], name='td_model_2')\n",
    "    model = connect_enc_dec([enc_1, enc_2], [dec_2, dec_1], name='td_model_2')\n",
    "    return td_model, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-277-e835098fe32c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtd_mod1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtd_mod1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "td_mod1, mod1 = model_1(5)\n",
    "td_mod1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-278-ce77f09dd7df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtd_dec_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mz_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_z_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtd_enc_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtd_enc_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_sampler'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_mean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "td_enc_1, enc_1 = encoder_1(k)\n",
    "td_dec_1, dec_1 = decoder_1(k)\n",
    "\n",
    "z_mean, log_z_var = td_enc_1.outputs\n",
    "sampler = Sampler(name=td_enc_1.name + '_sampler')\n",
    "batch = K.shape(z_mean)[0]\n",
    "latent_dim = K.shape(z_mean)[-1]\n",
    "td_samples = [sampler([z_mean[:, i, :], log_z_var[:, i, :]]) for i in range(k)]\n",
    "td_samples = [K.reshape(sample, (batch, 1, 100)) for sample in td_samples]\n",
    "td_samples = K.concatenate(td_samples, axis=1)\n",
    "print(td_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "(td_mod1, td_enc1, td_dec1), (mod1, enc1, dec1) = model_1(k)\n",
    "td_enc1.summary()\n",
    "\n",
    "# get sampling layers\n",
    "h1_sample = td_enc1.get_layer('td_enc_1').output\n",
    "h1_sample = sampler\n",
    "\n",
    "# get mean / std values\n",
    "z_mean = enc1.get_layer('enc_1_latent_mean').output\n",
    "log_z_var = enc1.get_layer('enc_1_log_latent_var').output\n",
    "\n",
    "def td_model_1_loss(y_true, y_pred):\n",
    "    # calculate log distributions (log_p_x_y := p(x|y))\n",
    "    log_q_h1_x = -0.5 * K.sum(log_z_var + K.exp(-log_z_var)*(h1_sample - z_mean)**2, axis=-1)\n",
    "    log_p_h1 = -0.5 * K.sum(K.square(h1_sample), axis=-1)\n",
    "    log_p_x_h1 = -K.sum(K.binary_crossentropy(y_true, y_pred), axis=(-1, -2))\n",
    "    #log_p_x_h1 = K.sum(y_true * K.log(y_pred) + (1-y_true) * K.log(1-y_pred), axis=(-1, -2))\n",
    "    \n",
    "    # calculate real distributions\n",
    "    q_h1_x = K.exp(log_q_h1_x)\n",
    "    p_h1 = K.exp(log_p_h1)\n",
    "    p_x_h1 = K.exp(log_p_x_h1)\n",
    "        \n",
    "    # calculate weights\n",
    "    log_weights = log_p_x_h1 + log_p_h1 - log_q_h1_x\n",
    "    #weights = K.softmax(log_weights, axis=1)\n",
    "    #weights = K.constant(K.get_value(weights))\n",
    "    #print(K.gradients(weights))\n",
    "    \n",
    "    # calculate loss\n",
    "    elbo = log_p_x_h1 + log_p_h1 - log_q_h1_x\n",
    "    elbo = K.mean(elbo, axis=1)\n",
    "    loss = -elbo\n",
    "    \n",
    "    return elbo\n",
    "\n",
    "td_mod1.compile(optimizer='adam', loss=td_model_1_loss)\n",
    "\n",
    "td_x_train = expand_dims(x_train, axis=1)\n",
    "td_x_train = repeat(td_x_train, k, axis=1)\n",
    "\n",
    "td_mod1.fit(x=td_x_train, y=td_x_train, batch_size=512, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_1_input (InputLayer)        (None, 28, 28)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_1_flatten (Flatten)         (None, 784)          0           enc_1_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "enc_1_hidden_1 (Dense)          (None, 200)          157000      enc_1_flatten[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "enc_1_hidden_2 (Dense)          (None, 200)          40200       enc_1_hidden_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "enc_1_latent_mean (Dense)       (None, 100)          20100       enc_1_hidden_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "enc_1_log_latent_var (Dense)    (None, 100)          20100       enc_1_hidden_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_1_sampler (Sampler)     (None, 100)          0           enc_1_latent_mean[0][0]          \n",
      "                                                                 enc_1_log_latent_var[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_1 (Model)               (None, 28, 28)       217984      encoder_1_sampler[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 455,384\n",
      "Trainable params: 455,384\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 13s 214us/step - loss: 246.2047\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 212.3893\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 203.6835\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 195.1265\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 180.7517\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 162.8604\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 152.9875\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 143.9213\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 137.8431\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 134.2065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x290858609b0>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "(td_mod1, td_enc1, td_dec1), (mod1, enc1, dec1) = model_1(k)\n",
    "mod1.summary()\n",
    "\n",
    "# get sampling layers\n",
    "h1_sample = mod1.get_layer('encoder_1_sampler').output\n",
    "\n",
    "# get mean / std values\n",
    "z_mean = mod1.get_layer('enc_1_latent_mean').output\n",
    "log_z_var = mod1.get_layer('enc_1_log_latent_var').output\n",
    "\n",
    "def model_1_loss(y_true, y_pred):\n",
    "    # calculate log distributions (log_p_x_y := p(x|y))\n",
    "    log_q_h1_x = -0.5 * K.sum(log_z_var + K.exp(-log_z_var)*(h1_sample - z_mean)**2, axis=-1)\n",
    "    log_p_h1 = -0.5 * K.sum(K.square(h1_sample), axis=-1)\n",
    "    log_p_x_h1 = -K.sum(K.binary_crossentropy(y_true, y_pred), axis=(-1, -2))\n",
    "    #log_p_x_h1 = K.sum(y_true * K.log(y_pred) + (1-y_true) * K.log(1-y_pred), axis=(-1, -2))\n",
    "        \n",
    "    # calculate weights\n",
    "    log_weights = log_p_x_h1 + log_p_h1 - log_q_h1_x\n",
    "    #weights = K.softmax(log_weights, axis=1)\n",
    "    #weights = K.constant(K.get_value(weights))\n",
    "    #print(K.gradients(weights))\n",
    "    \n",
    "    # calculate loss\n",
    "    #elbo = K.mean((log_p_x_h1 + log_p_h1 - log_q_h1_x), axis=1)\n",
    "    elbo = log_p_x_h1 + log_p_h1 - log_q_h1_x\n",
    "    loss = -elbo\n",
    "        \n",
    "    return loss\n",
    "\n",
    "mod1.compile(optimizer='adam', loss=model_1_loss)\n",
    "\n",
    "mod1.fit(x=x_train, y=x_train, batch_size=512, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mod1.predict(x_train[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = K.constant(x_train[0:1])\n",
    "\n",
    "y_true = a\n",
    "y_pred = mod1(a)\n",
    "\n",
    "h1_sample = mod1.get_layer('encoder_1_sampler').output\n",
    "m = Model(mod1.inputs, h1_sample)\n",
    "h1_sample = m(a)\n",
    "z_mean = mod1.get_layer('enc_1_latent_mean').output\n",
    "log_z_var = mod1.get_layer('enc_1_log_latent_var').output\n",
    "\n",
    "# calculate log distributions (log_p_x_y := p(x|y))\n",
    "log_q_h1_x = -0.5 * K.sum(log_z_var + K.exp(-log_z_var) * K.square(h1_sample - z_mean), axis=-1)\n",
    "log_p_h1 = -0.5 * K.sum(K.square(h1_sample), axis=-1)\n",
    "log_p_x_h1 = -K.sum(K.binary_crossentropy(y_true, y_pred), axis=(-1, -2))\n",
    "\n",
    "x = mod1.inputs[0]\n",
    "\n",
    "sampler = Model(x, Sampler()([z_mean, log_z_var]))\n",
    "enc = Model(x, [z_mean, log_z_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean = K.eval(enc(a)[0])\n",
    "log_z_var = K.eval(enc(a)[1])\n",
    "h1_sample = K.eval(h1_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_q_h1_x = -0.5 * np.sum(log_z_var + np.exp(-log_z_var) * (h1_sample - z_mean)**2, axis=-1)\n",
    "log_p_h1 = -0.5 * np.sum(np.square(z_mean), axis=-1)\n",
    "log_p_x_h1 = K.eval(-K.sum(K.binary_crossentropy(y_true, y_pred), axis=(-1, -2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-13.837625], dtype=float32)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-log_q_h1_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.898342], dtype=float32)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_p_h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-164.6179], dtype=float32)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_p_x_h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-184.35388], dtype=float32)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elbo = log_p_x_h1 + log_p_h1 - log_q_h1_x\n",
    "elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([184.35388], dtype=float32)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x290703cfcc0>"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = td_mod1.predict(td_x_train[0:1])\n",
    "imshow(a[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
