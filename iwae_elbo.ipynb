{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, Layer, TimeDistributed, Concatenate, Lambda\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "from vae_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('./data/iwae/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "def preprocess(data):\n",
    "    x, y = data\n",
    "    x = x.reshape((len(x), 28, 28))\n",
    "    x = x/255.\n",
    "    #y = to_categorical(y, 10) #don't need to categorise y\n",
    "    return x, y\n",
    "\n",
    "train, test = mnist.load_data()\n",
    "\n",
    "x_train, y_train = preprocess(train)\n",
    "x_test, y_test = preprocess(test)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a layer that samples a latent variable given a mean and standard deviation\n",
    "\n",
    "class Sampler(Layer):\n",
    "    \n",
    "    def __init__(self, always_sample=False, **kwargs):\n",
    "        self.stddev = 1\n",
    "        self.always_sample = always_sample\n",
    "        super(Sampler, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, x, training=None):\n",
    "        assert isinstance(x, list)\n",
    "        z_mean, log_z_var = x\n",
    "        z_std = K.exp(log_z_var/2)\n",
    "        \n",
    "        # sample epsilon from N(0, stddev)\n",
    "        shape = K.shape(z_std)\n",
    "        epsilon = K.random_normal(shape, mean=0, stddev=self.stddev)\n",
    "        z_sample = z_mean + z_std * epsilon\n",
    "        \n",
    "        if self.always_sample:\n",
    "            return z_sample\n",
    "        else:\n",
    "            return K.in_train_phase(z_sample, z_mean, training=training)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        assert input_shape[0] == input_shape[1]\n",
    "        return input_shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(k, latent_dim=100, hidden_dim=200, always_sample=False):\n",
    "    \"\"\"Define model 1 as the single stochastic model in [1]\n",
    "    \n",
    "    k: int\n",
    "        Number of k samples in the IWAE model, or the number of MC samples\n",
    "        for the VAE model.\n",
    "        \n",
    "    latent_dim: int, optional\n",
    "        Dimensionality of the latent space, default = 100.\n",
    "        \n",
    "    hidden_dim: int, optional\n",
    "        Number of hidden units for the dense layers.\n",
    "        \n",
    "    always_sample: boolean, optional\n",
    "        Whether to always sample from the posterior distribution, or only\n",
    "        during training. Default = False.\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    \n",
    "    [1] Burda Y, Grosse R, Salakhutdinov R. Importance weighted autoencoders. \n",
    "        arXiv preprint arXiv:1509.00519. 2015 Sep 1.\n",
    "    \"\"\"\n",
    "    # encoder shared layers\n",
    "    enc_hid_1 = Dense(hidden_dim, activation='tanh', name='enc_1_hidden_1')\n",
    "    enc_hid_2 = Dense(hidden_dim, activation='tanh', name='enc_1_hidden_2')\n",
    "    z_mean = Dense(latent_dim, name='enc_1_latent_mean')\n",
    "    log_z_var = Dense(latent_dim, name='enc_1_log_latent_var')\n",
    "    sampler = Sampler(always_sample, name='z1_sampler')\n",
    "    \n",
    "    # decoder shared layers\n",
    "    dec_hid_1 = Dense(hidden_dim, activation='tanh', name='dec_1_hidden_1')\n",
    "    dec_hid_2 = Dense(hidden_dim, activation='tanh', name='dec_1_hidden_2')\n",
    "    bernoulli_mean = Dense(28*28, activation='sigmoid', name='dec_1_mean')\n",
    "    reshape = Reshape((28, 28), name='dec_1_output')\n",
    "    \n",
    "    # single pass model\n",
    "    x = Input(shape=(28, 28), name='enc_1_input')\n",
    "    y = Flatten(name='enc_1_flatten')(x)\n",
    "    y = enc_hid_1(y)\n",
    "    y = enc_hid_2(y)\n",
    "    mu = z_mean(y)\n",
    "    log_var = log_z_var(y)\n",
    "    z1 = sampler([z_mean(y), log_z_var(y)])\n",
    "    y = dec_hid_1(z1)\n",
    "    y = dec_hid_2(y)\n",
    "    y = bernoulli_mean(y)\n",
    "    y = reshape(y)\n",
    "    \n",
    "    model = Model(x, y, name='model_1')\n",
    "    \n",
    "    # k forward passes - start from first sampling layer\n",
    "    k_z1 = [sampler([mu, log_var]) for i in range(k)]\n",
    "    k_y = [dec_hid_1(z1) for z1 in k_z1]\n",
    "    k_y = [dec_hid_2(y) for y in k_y]\n",
    "    k_y = [bernoulli_mean(y) for y in k_y]\n",
    "    k_y = [reshape(y) for y in k_y]\n",
    "    \n",
    "    return model, mu, log_var, k_z1, k_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_1(k, latent_dim=100, epochs=50, batch_size=512, train='both'):\n",
    "    \"\"\"Trains the first model defined in [1].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    k: int\n",
    "        Number of forward passes for the IWAE model, and number of MC samples\n",
    "        in the VAE model\n",
    "        \n",
    "    latent_dim: int, optional\n",
    "        Dimensionality of the latent space, default = 100.\n",
    "        \n",
    "    epochs: int, optional\n",
    "        Number of epochs to train over, default=50.\n",
    "        \n",
    "    batch_size: int, optional\n",
    "        Batch size of training sample, default=512.\n",
    "    \n",
    "    train: string, optional\n",
    "        Indicates which model to train, either 'iwae', 'vae', or 'both'. Default = 'both'\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    model: keras model or tuple\n",
    "        If train is either 'iwae' or 'vae', returns a single trained keras model. Otherwise\n",
    "        returns a tuple of keras models, where model=(iwae_model, vae_model).\n",
    "        \n",
    "    hist: keras history or tuple\n",
    "        If train is either 'iwae' or 'vae', returns a single history object containing the\n",
    "        training history of the model. Otherwise returns a tuple of history objects.\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    \n",
    "    [1] Burda Y, Grosse R, Salakhutdinov R. Importance weighted autoencoders. \n",
    "        arXiv preprint arXiv:1509.00519. 2015 Sep 1.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    hists = []\n",
    "    \n",
    "    if train.lower() == 'iwae' or train.lower() == 'both':\n",
    "        # IWAE training\n",
    "        print('Training IWAE model')\n",
    "        print('-------------------')\n",
    "\n",
    "        iwae_model, z_mean, log_z_var, z1_samples, out_samples = model_1(k, latent_dim)\n",
    "        \n",
    "        def iwae_loss(y_true, y_pred):\n",
    "            # calculate log distributions (log_p_x_y := p(x|y))\n",
    "            log_weights = []\n",
    "            for i in range(k):\n",
    "                log_q_h1_x = -0.5 * K.sum(log_z_var + K.exp(-log_z_var)*(z1_samples[i] - z_mean)**2, axis=-1)\n",
    "                log_p_h1 = -0.5 * K.sum(K.square(z1_samples[i]), axis=-1)\n",
    "                log_p_x_h1 = -K.sum(K.binary_crossentropy(y_true, out_samples[i]), axis=(-1, -2))\n",
    "\n",
    "                # calculate weights\n",
    "                log_weight = log_p_x_h1 + log_p_h1 - log_q_h1_x\n",
    "                log_weights.append(log_weight)\n",
    "                \n",
    "            max_log_weights = K.max(log_weights, axis=0, keepdims=True) # for numerical stability\n",
    "            weights = K.exp(log_weights - max_log_weights)\n",
    "            elbo = 1/k * K.sum(weights, axis=0)\n",
    "            elbo = K.log(elbo) + max_log_weights\n",
    "            \n",
    "            # uncomment to add second order term to the elbo\n",
    "            #v_sqr = K.square(log_weights)\n",
    "            #v_sqr = 1/k * K.sum(v_sqr, axis=0)\n",
    "            #second_ord = -1/2 * (v_sqr - elbo**2)\n",
    "            #elbo = elbo + second_ord\n",
    "            \n",
    "            loss = -elbo\n",
    "\n",
    "            return loss\n",
    "        \n",
    "        iwae_model.compile(optimizer='adam', loss=iwae_loss)\n",
    "        hist = iwae_model.fit(x_train, x_train, batch_size=batch_size, epochs=epochs)\n",
    "        \n",
    "        out.append(iwae_model)\n",
    "        hists.append(hist)\n",
    "\n",
    "        model_path = './iwae_model_1_k_%d_dim_%d.weights' %(k, latent_dim)\n",
    "        iwae_model.save_weights(model_path)\n",
    "        \n",
    "        \n",
    "    if train.lower() == 'vae' or train.lower() == 'both':\n",
    "        # VAE training\n",
    "        if train.lower() == 'both':\n",
    "            print('\\n')\n",
    "        print('Training VAE model')\n",
    "        print('------------------')\n",
    "        \n",
    "        vae_model, z_mean, log_z_var, z1_samples, out_samples = model_1(k, latent_dim)\n",
    "        \n",
    "        def vae_loss(y_true, y_pred):\n",
    "            loss = 0\n",
    "            elbos = []\n",
    "            for i in range(k):\n",
    "                log_q_h1_x = -0.5 * K.sum(log_z_var + K.exp(-log_z_var)*(z1_samples[i] - z_mean)**2, axis=-1)\n",
    "                log_p_h1 = -0.5 * K.sum(K.square(z1_samples[i]), axis=-1)\n",
    "                log_p_x_h1 = -K.sum(K.binary_crossentropy(y_true, out_samples[i]), axis=(-1, -2))\n",
    "\n",
    "                elbo = log_p_x_h1 + log_p_h1 - log_q_h1_x\n",
    "                elbos.append(elbo)\n",
    "                loss -= elbo\n",
    "            \n",
    "            loss = loss / k\n",
    "            \n",
    "            # uncomment to see what happens when we add the second order term to the vae loss\n",
    "            #elbo = 1/k * K.sum(elbos, axis=0)\n",
    "            #v_sqr = K.square(elbos)\n",
    "            #v_sqr = 1/k * K.sum(v_sqr, axis=0)\n",
    "            #second_ord = -1/2 * (v_sqr - elbo**2)\n",
    "            #elbo = elbo + second_ord\n",
    "            \n",
    "            #loss = -elbo\n",
    "            \n",
    "            return loss\n",
    "\n",
    "        vae_model.compile(optimizer='adam', loss=vae_loss)\n",
    "        hist = vae_model.fit(x_train, x_train, batch_size=batch_size, epochs=epochs)\n",
    "        \n",
    "        out.append(vae_model)\n",
    "        hists.append(hist)\n",
    "\n",
    "        model_path = './vae_model_1_k_%d_dim_%d.weights' %(k, latent_dim)\n",
    "        vae_model.save_weights(model_path)\n",
    "    \n",
    "    \n",
    "    if train.lower() not in ['both', 'vae', 'iwae']:\n",
    "        print('Set parameter train to \"both\", \"vae\", or \"iwae\".')\n",
    "        return\n",
    "    \n",
    "    if train.lower() == 'both':\n",
    "        return out, hists\n",
    "    return out[0], hists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch trainer\n",
    "\n",
    "def train_for_ks(ks, latent_dim=100, train='both'):\n",
    "    \"\"\"Trains a model for each k in a list of ks.\"\"\"\n",
    "    ks = np.asarray(ks)\n",
    "    print('TRAINING')\n",
    "    for k in ks:\n",
    "        print('\\n')\n",
    "        print('-------------------')\n",
    "        print('k = %d' %k)\n",
    "        print('latent_dim = %d' %latent_dim)\n",
    "        print('-------------------\\n')\n",
    "        train_model_1(k, latent_dim, train=train)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ks = [1, 5, 10, 20, 30, 40, 50]\n",
    "ks = [1, 5, 10, 20, 30, 40, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "\n",
      "\n",
      "-------------------\n",
      "k = 1\n",
      "latent_dim = 100\n",
      "-------------------\n",
      "\n",
      "Training IWAE model\n",
      "-------------------\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 245.8504\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 206.6493\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 199.1927\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 191.3558\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 175.1117\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 163.9559\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 153.7603\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 145.7763\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 140.1189\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 135.8921\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 132.6237\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 130.0571: 0s - loss: 130.063\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 127.9566\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 126.1782\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 124.2204\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 122.5306\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 121.0582\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 119.8283\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 118.7123\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 117.7735\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 116.8152\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 116.0072\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 115.2665\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 114.6588\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 113.9525\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 113.5259\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 112.8846\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 112.4124\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 111.9035\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 111.4356\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 111.0829\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 110.6332\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 110.3640\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 109.8556: 0s - loss: 109.848\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 109.5715\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 109.1938\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 108.9170\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 108.6203\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 108.3293\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 108.0260\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 107.7501\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 107.4948\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 107.3134\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 107.0114\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 106.7789\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 106.5594\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 106.3085\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 106.1169\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 105.8672\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 105.6750\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-------------------\n",
      "k = 5\n",
      "latent_dim = 100\n",
      "-------------------\n",
      "\n",
      "Training IWAE model\n",
      "-------------------\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 233.7545\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 13s 214us/step - loss: 185.5802\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 15s 251us/step - loss: 164.8581\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 154.9924\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 146.9084\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 140.6409\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 15s 251us/step - loss: 134.9904\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 15s 253us/step - loss: 130.4670\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 126.8905\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 123.9379\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 121.5537\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 119.4873\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 117.7731\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 116.4382\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - ETA: 0s - loss: 115.122 - 15s 254us/step - loss: 115.1229\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 15s 253us/step - loss: 113.9335\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 15s 253us/step - loss: 112.8884\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 111.8724\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 15s 251us/step - loss: 111.0302\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 15s 255us/step - loss: 110.2583\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 15s 251us/step - loss: 109.4734\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 15s 253us/step - loss: 108.9134\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 15s 257us/step - loss: 108.2776\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 30s 497us/step - loss: 107.7147\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 17s 289us/step - loss: 107.2288\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 106.7831\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 106.3522\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 105.9371\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 15s 251us/step - loss: 105.5841\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 105.2055\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 104.8613\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 104.5663\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 15s 251us/step - loss: 104.2538\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 103.9734\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 15s 253us/step - loss: 103.7617\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 27s 443us/step - loss: 103.4235\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 28s 464us/step - loss: 103.2345\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 15s 251us/step - loss: 102.9979\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 15s 251us/step - loss: 102.7852\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 102.5838\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 15s 251us/step - loss: 102.3599\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 102.2195\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 15s 251us/step - loss: 102.0640\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 101.8195\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 15s 251us/step - loss: 101.6556\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 101.5246\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 101.3971\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 101.2658\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 101.0830\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 101.0174\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-------------------\n",
      "k = 10\n",
      "latent_dim = 100\n",
      "-------------------\n",
      "\n",
      "Training IWAE model\n",
      "-------------------\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 29s 488us/step - loss: 231.8766\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 27s 454us/step - loss: 177.9265\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 160.0542\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 27s 454us/step - loss: 151.4053\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 40s 667us/step - loss: 145.4289\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 41s 679us/step - loss: 140.2967\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 135.0580\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 27s 456us/step - loss: 129.7239\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 125.5736\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 27s 456us/step - loss: 122.3304\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 40s 674us/step - loss: 119.8735\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 42s 694us/step - loss: 117.8716\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 27s 458us/step - loss: 116.2029\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 27s 457us/step - loss: 114.8459\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 27s 456us/step - loss: 113.6720\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 112.6242\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 27s 456us/step - loss: 111.6585\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 27s 454us/step - loss: 110.7315\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 109.9856\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 27s 456us/step - loss: 109.1442\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 108.4690\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 107.8378\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 107.2406\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 106.6315\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 106.1433\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 105.6447\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 105.1753\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 104.7960\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 104.4012\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 27s 456us/step - loss: 104.0805\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 103.7108\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 48s 796us/step - loss: 103.4469\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 46s 769us/step - loss: 103.0940\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 27s 454us/step - loss: 102.8524\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 37s 625us/step - loss: 102.6431\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 48s 802us/step - loss: 102.4032\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 102.1594\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 27s 456us/step - loss: 101.9393\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 47s 784us/step - loss: 101.7890\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 42s 693us/step - loss: 101.5139\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 101.4050\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 27s 456us/step - loss: 101.2851\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 101.0505\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 100.8644\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 27s 455us/step - loss: 100.7330\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 27s 456us/step - loss: 100.6270\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 37s 608us/step - loss: 100.4796\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 57s 945us/step - loss: 100.3272\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 38s 635us/step - loss: 100.2141\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 27s 456us/step - loss: 100.1089\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-------------------\n",
      "k = 20\n",
      "latent_dim = 100\n",
      "-------------------\n",
      "\n",
      "Training IWAE model\n",
      "-------------------\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 51s 852us/step - loss: 229.6635\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 52s 863us/step - loss: 170.8518\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 52s 864us/step - loss: 154.8354\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 52s 864us/step - loss: 146.1470\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 52s 864us/step - loss: 140.7101\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 52s 866us/step - loss: 136.4033\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 84s 1ms/step - loss: 131.9597\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 52s 864us/step - loss: 127.5309\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 52s 864us/step - loss: 123.5956\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 52s 867us/step - loss: 120.6958\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 118.3594\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 56s 931us/step - loss: 116.3340\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 70s 1ms/step - loss: 114.7403\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 91s 2ms/step - loss: 113.3384\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 52s 866us/step - loss: 112.2088\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 111.1991\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 90s 1ms/step - loss: 110.2177\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 65s 1ms/step - loss: 109.3594\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 94s 2ms/step - loss: 108.6708\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 71s 1ms/step - loss: 107.9518\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 107.3015\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 59s 982us/step - loss: 106.6920\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 106.0831\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 105.5080\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 53s 891us/step - loss: 105.0954\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 104.6128\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 100s 2ms/step - loss: 104.1796\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 72s 1ms/step - loss: 103.7715\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 103.4392\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 103.0961\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 102.7643\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 87s 1ms/step - loss: 102.5133\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 102.2035\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 53s 890us/step - loss: 101.9924\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 101.7492\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 101.5271\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 101.2585\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 80s 1ms/step - loss: 101.0779\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 59s 976us/step - loss: 100.8770\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 100.7388\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 100.5476\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 100.3875\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 100.2220\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 100.0769\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 106s 2ms/step - loss: 99.9023\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 32s 526us/step - loss: 99.8355\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 64s 1ms/step - loss: 99.7372\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 99.5068\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 99.3666\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 109s 2ms/step - loss: 99.3023\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-------------------\n",
      "k = 30\n",
      "latent_dim = 100\n",
      "-------------------\n",
      "\n",
      "Training IWAE model\n",
      "-------------------\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 167s 3ms/step - loss: 228.7747\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 161s 3ms/step - loss: 169.3235\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 151s 3ms/step - loss: 153.5090\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 144.9223\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 76s 1ms/step - loss: 139.7682\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 135.4496\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 128s 2ms/step - loss: 131.2401\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 161s 3ms/step - loss: 127.1324\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 123.1346\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 94s 2ms/step - loss: 120.0147\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 117.6500\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 115.7371\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 114.0694\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 112.7233\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 105s 2ms/step - loss: 111.6414\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 82s 1ms/step - loss: 110.6070\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 109.7120\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 108.8775\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 108.1738\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 58s 965us/step - loss: 107.4665\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 42s 705us/step - loss: 106.7587\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 42s 702us/step - loss: 106.1318\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 105s 2ms/step - loss: 105.5133\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 163s 3ms/step - loss: 104.9891\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 163s 3ms/step - loss: 104.5325\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 163s 3ms/step - loss: 104.0920\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 163s 3ms/step - loss: 103.7150\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 163s 3ms/step - loss: 103.2745\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 163s 3ms/step - loss: 102.9664\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 163s 3ms/step - loss: 102.6085\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 163s 3ms/step - loss: 102.2943\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 157s 3ms/step - loss: 102.0986\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 44s 734us/step - loss: 101.8423\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 42s 708us/step - loss: 101.5211\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 42s 701us/step - loss: 101.3240\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 41s 686us/step - loss: 101.0669\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 40s 670us/step - loss: 100.8479\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 41s 681us/step - loss: 100.6521\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 41s 688us/step - loss: 100.4677\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 72s 1ms/step - loss: 100.2828\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 47s 779us/step - loss: 100.1242\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 59s 989us/step - loss: 99.9792\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 99.7875\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 99.6596\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 99.5044\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 99.3802\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 99.2692\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 99.1403\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 99.0994\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 98.8782\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-------------------\n",
      "k = 40\n",
      "latent_dim = 100\n",
      "-------------------\n",
      "\n",
      "Training IWAE model\n",
      "-------------------\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 94s 2ms/step - loss: 228.1860\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 171.6365\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 153.4265\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 144.4284\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 138.9821\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 134.1970\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 129.7548\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 125.5207\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 121.8147\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 119.0718\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 116.9574\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 115.0780\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 121s 2ms/step - loss: 113.6142\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 112.2503\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 111.0497\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 101s 2ms/step - loss: 109.9945\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 109.0183\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 108.2418\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 147s 2ms/step - loss: 107.4372\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 106.7065\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 165s 3ms/step - loss: 106.1032\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 105.5099\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 105.0175\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 104.5002\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 104.0713\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 103.6328\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 103.2609\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 102.8259\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 119s 2ms/step - loss: 102.5334\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 102.2564\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 101.9259\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 120s 2ms/step - loss: 101.6068\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 101.3684\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 101.1191\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 100.9376\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 100.6740\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 121s 2ms/step - loss: 100.5310\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 100.2865\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 128s 2ms/step - loss: 100.1569\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 104s 2ms/step - loss: 99.9501\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 99.7703\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 99.6348\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 99.5025\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 99.3583\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 142s 2ms/step - loss: 99.1907\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 99.1135\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 123s 2ms/step - loss: 98.9520\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 101s 2ms/step - loss: 98.8124\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 153s 3ms/step - loss: 98.7225\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 124s 2ms/step - loss: 98.6086\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-------------------\n",
      "k = 50\n",
      "latent_dim = 100\n",
      "-------------------\n",
      "\n",
      "Training IWAE model\n",
      "-------------------\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 137s 2ms/step - loss: 229.1100\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 169.2553\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 152.7708\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 143.9196\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 177s 3ms/step - loss: 138.4702\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 132s 2ms/step - loss: 133.9518\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 129.5333\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 125.5106\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 122.1703\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 119.3286\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 116.8767\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 190s 3ms/step - loss: 114.8433\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 151s 3ms/step - loss: 113.1516\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 227s 4ms/step - loss: 111.7970\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 256s 4ms/step - loss: 110.5911\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 151s 3ms/step - loss: 109.5735\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 222s 4ms/step - loss: 108.7608\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 165s 3ms/step - loss: 107.9139\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 107.1276\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 140s 2ms/step - loss: 106.4828\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 153s 3ms/step - loss: 105.8826\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 150s 2ms/step - loss: 105.2588\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 173s 3ms/step - loss: 104.6834\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 204s 3ms/step - loss: 104.1910\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 145s 2ms/step - loss: 103.7388\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 231s 4ms/step - loss: 103.3292\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 155s 3ms/step - loss: 102.9101\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 202s 3ms/step - loss: 102.5723\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 102.1983\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 245s 4ms/step - loss: 101.8627\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 173s 3ms/step - loss: 101.5887\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 166s 3ms/step - loss: 101.3428\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 187s 3ms/step - loss: 101.0468\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 215s 4ms/step - loss: 100.8580\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 264s 4ms/step - loss: 100.6269\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 208s 3ms/step - loss: 100.4012\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 149s 2ms/step - loss: 100.1715\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 99.9727\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 99.7986\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 99.6367\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 99.4480\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 172s 3ms/step - loss: 99.3120\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 193s 3ms/step - loss: 99.2049\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 222s 4ms/step - loss: 99.0603\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 98.8845\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 98.7717\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 98.6339\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 98.4921\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 98.4506\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 126s 2ms/step - loss: 98.2992\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# uncomment to train all models (will take long!)\n",
    "#train_for_ks(ks, train='iwae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IWAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iwae model and get relevant predictions\n",
    "\n",
    "k = 100\n",
    "\n",
    "load_k = 10\n",
    "latent_dim = 100\n",
    "\n",
    "iwae_path = 'iwae_model_1_k_%d_dim_%d.weights' %(load_k, latent_dim)\n",
    "    \n",
    "# load model\n",
    "iwae_model, z_mean, log_z_var, z1_samples, out_samples = model_1(k, latent_dim, always_sample=True)\n",
    "iwae_model.load_weights(iwae_path, by_name=True)\n",
    "\n",
    "k_iwae_model = Model(iwae_model.input, out_samples)\n",
    "k_z_samples = Model(iwae_model.input, z1_samples)\n",
    "enc_model = Model(iwae_model.input, [z_mean, log_z_var])\n",
    "\n",
    "#iwae_model.compile(optimizer='adam', loss=iwae_loss)\n",
    "\n",
    "x_preds = k_iwae_model.predict(x_test, batch_size=512)\n",
    "z1_samples = k_z_samples.predict(x_test, batch_size=512)\n",
    "z_mean, log_z_var = enc_model.predict(x_test, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iwae_elbo(y_true, y_pred):\n",
    "    # calculate log distributions (log_p_x_y := p(x|y))\n",
    "    elbo = []\n",
    "    log_weights = []\n",
    "    for i in range(k):\n",
    "        log_q_h1_x = -0.5 * np.sum(log_z_var + np.exp(-log_z_var)*(z1_samples[i] - z_mean)**2, axis=-1)\n",
    "        log_p_h1 = -0.5 * np.sum(z1_samples[i]**2, axis=-1)\n",
    "        log_p_x_h1 = np.sum(y_true * np.log(y_pred[i]) + (1 - y_true) * np.log(1 - y_pred[i]), axis=(-1, -2))\n",
    "\n",
    "        # calculate weights\n",
    "        log_weight = log_p_x_h1 + log_p_h1 - log_q_h1_x\n",
    "        log_weights.append(log_weight)\n",
    "    \n",
    "    weights = np.exp(log_weights - np.max(log_weights, axis=1, keepdims=True))\n",
    "    weights = weights / sum(weights, axis=0)\n",
    "    var = np.var(log_weights, axis=0)\n",
    "    elbo = np.sum(weights * log_weights, axis=0)\n",
    "    \n",
    "    #max_log_weights = np.max(log_weights, axis=0, keepdims=True)\n",
    "    #weights = np.exp(log_weights - max_log_weights)\n",
    "    #elbo = 1/k * np.sum(weights, axis=0)\n",
    "    #elbo = np.log(elbo) + max_log_weights\n",
    "    #loss = -elbo\n",
    "\n",
    "    return elbo, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo, var = iwae_elbo(x_test, x_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ -67.55014334, -101.85257523,  -36.07956777, ...,  -70.2541252 ,\n",
       "        -100.16458429, -127.49486745]),\n",
       " -90.51693533571992,\n",
       " array([ 50.56878296, 166.69092112,  92.38577227, ...,  55.90923185,\n",
       "         94.97351878, 113.90322716]),\n",
       " 74.12739445854317)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elbo, mean(elbo), var, mean(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of active units\n",
    "A_u = np.var(z_mean, axis=0)\n",
    "sum(log(A_u) >= -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iwae model and get relevant predictions\n",
    "\n",
    "k = 100\n",
    "\n",
    "load_k = 50\n",
    "latent_dim = 100\n",
    "\n",
    "vae_path = 'vae_model_1_k_%d_dim_%d.weights' %(load_k, latent_dim)\n",
    "    \n",
    "# load model\n",
    "vae_model, z_mean, log_z_var, z1_samples, out_samples = model_1(k, latent_dim, always_sample=True)\n",
    "vae_model.load_weights(vae_path, by_name=True)\n",
    "\n",
    "k_vae_model = Model(vae_model.input, out_samples)\n",
    "k_z_samples = Model(vae_model.input, z1_samples)\n",
    "enc_model = Model(vae_model.input, [z_mean, log_z_var])\n",
    "\n",
    "#vae_model.compile(optimizer='adam', loss=vae_loss)\n",
    "\n",
    "x_preds = k_vae_model.predict(x_test, batch_size=512)\n",
    "z1_samples = k_z_samples.predict(x_test, batch_size=512)\n",
    "z_mean, log_z_var = enc_model.predict(x_test, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_elbo(y_true, y_pred):\n",
    "    # calculate log distributions (log_p_x_y := p(x|y))\n",
    "    elbos = []\n",
    "    for i in range(k):\n",
    "        log_q_h1_x = -0.5 * np.sum(log_z_var + np.exp(-log_z_var)*(z1_samples[i] - z_mean)**2, axis=-1)\n",
    "        log_p_h1 = -0.5 * np.sum(z1_samples[i]**2, axis=-1)\n",
    "        log_p_x_h1 = np.sum(y_true * np.log(y_pred[i]) + (1 - y_true) * np.log(1 - y_pred[i]), axis=(-1, -2))\n",
    "        \n",
    "        elbos.append(log_p_x_h1 + log_p_h1 - log_q_h1_x)\n",
    "    \n",
    "    var = np.var(elbos, axis=0)\n",
    "    elbo = np.mean(elbos, axis=0)\n",
    "\n",
    "    return elbo, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo, var = vae_elbo(x_test, x_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ -78.82926359, -119.65482459,  -45.0896923 , ...,  -87.83928196,\n",
       "        -115.23762985, -133.43586886]),\n",
       " -103.16403082185722,\n",
       " array([15.11498572, 21.73034528, 11.11491241, ..., 24.11814075,\n",
       "        20.55749262, 25.7175197 ]),\n",
       " 20.583475000499245)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elbo, mean(elbo), var, mean(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of active units\n",
    "A_u = np.var(z_mean, axis=0)\n",
    "sum(log(A_u) >= -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
