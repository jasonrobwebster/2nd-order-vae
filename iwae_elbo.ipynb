{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, Layer, TimeDistributed, Concatenate, Lambda\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "from vae_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('./data/iwae/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "def preprocess(data):\n",
    "    x, y = data\n",
    "    x = x.reshape((len(x), 28, 28))\n",
    "    x = x/255.\n",
    "    #y = to_categorical(y, 10) #don't need to categorise y\n",
    "    return x, y\n",
    "\n",
    "train, test = mnist.load_data()\n",
    "\n",
    "x_train, y_train = preprocess(train)\n",
    "x_test, y_test = preprocess(test)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a layer that samples a latent variable given a mean and standard deviation\n",
    "\n",
    "class Sampler(Layer):\n",
    "    \n",
    "    def __init__(self, always_sample=False, **kwargs):\n",
    "        self.stddev = 1\n",
    "        self.always_sample = always_sample\n",
    "        super(Sampler, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, x, training=None):\n",
    "        assert isinstance(x, list)\n",
    "        z_mean, log_z_var = x\n",
    "        z_std = K.exp(log_z_var/2)\n",
    "        \n",
    "        # sample epsilon from N(0, stddev)\n",
    "        shape = K.shape(z_std)\n",
    "        epsilon = K.random_normal(shape, mean=0, stddev=self.stddev)\n",
    "        z_sample = z_mean + z_std * epsilon\n",
    "        \n",
    "        if self.always_sample:\n",
    "            return z_sample\n",
    "        else:\n",
    "            return K.in_train_phase(z_sample, z_mean, training=training)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        assert input_shape[0] == input_shape[1]\n",
    "        return input_shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(k, latent_dim=100, hidden_dim=200, always_sample=False):\n",
    "    \"\"\"Define model 1 as the single stochastic model in [1]\n",
    "    \n",
    "    k: int\n",
    "        Number of k samples in the IWAE model, or the number of MC samples\n",
    "        for the VAE model.\n",
    "        \n",
    "    latent_dim: int, optional\n",
    "        Dimensionality of the latent space, default = 100.\n",
    "        \n",
    "    hidden_dim: int, optional\n",
    "        Number of hidden units for the dense layers.\n",
    "        \n",
    "    always_sample: boolean, optional\n",
    "        Whether to always sample from the posterior distribution, or only\n",
    "        during training. Default = False.\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    \n",
    "    [1] Burda Y, Grosse R, Salakhutdinov R. Importance weighted autoencoders. \n",
    "        arXiv preprint arXiv:1509.00519. 2015 Sep 1.\n",
    "    \"\"\"\n",
    "    # encoder shared layers\n",
    "    enc_hid_1 = Dense(hidden_dim, activation='tanh', name='enc_1_hidden_1')\n",
    "    enc_hid_2 = Dense(hidden_dim, activation='tanh', name='enc_1_hidden_2')\n",
    "    z_mean = Dense(latent_dim, name='enc_1_latent_mean')\n",
    "    log_z_var = Dense(latent_dim, name='enc_1_log_latent_var')\n",
    "    sampler = Sampler(always_sample, name='z1_sampler')\n",
    "    \n",
    "    # decoder shared layers\n",
    "    dec_hid_1 = Dense(hidden_dim, activation='tanh', name='dec_1_hidden_1')\n",
    "    dec_hid_2 = Dense(hidden_dim, activation='tanh', name='dec_1_hidden_2')\n",
    "    bernoulli_mean = Dense(28*28, activation='sigmoid', name='dec_1_mean')\n",
    "    reshape = Reshape((28, 28), name='dec_1_output')\n",
    "    \n",
    "    # single pass model\n",
    "    x = Input(shape=(28, 28), name='enc_1_input')\n",
    "    y = Flatten(name='enc_1_flatten')(x)\n",
    "    y = enc_hid_1(y)\n",
    "    y = enc_hid_2(y)\n",
    "    mu = z_mean(y)\n",
    "    log_var = log_z_var(y)\n",
    "    z1 = sampler([z_mean(y), log_z_var(y)])\n",
    "    y = dec_hid_1(z1)\n",
    "    y = dec_hid_2(y)\n",
    "    y = bernoulli_mean(y)\n",
    "    y = reshape(y)\n",
    "    \n",
    "    model = Model(x, y, name='model_1')\n",
    "    \n",
    "    # k forward passes - start from first sampling layer\n",
    "    k_z1 = [sampler([mu, log_var]) for i in range(k)]\n",
    "    k_y = [dec_hid_1(z1) for z1 in k_z1]\n",
    "    k_y = [dec_hid_2(y) for y in k_y]\n",
    "    k_y = [bernoulli_mean(y) for y in k_y]\n",
    "    k_y = [reshape(y) for y in k_y]\n",
    "    \n",
    "    return model, mu, log_var, k_z1, k_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_1(k, latent_dim=100, epochs=50, batch_size=512, train='both'):\n",
    "    \"\"\"Trains the first model defined in [1].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    k: int\n",
    "        Number of forward passes for the IWAE model, and number of MC samples\n",
    "        in the VAE model\n",
    "        \n",
    "    latent_dim: int, optional\n",
    "        Dimensionality of the latent space, default = 100.\n",
    "        \n",
    "    epochs: int, optional\n",
    "        Number of epochs to train over, default=50.\n",
    "        \n",
    "    batch_size: int, optional\n",
    "        Batch size of training sample, default=512.\n",
    "    \n",
    "    train: string, optional\n",
    "        Indicates which model to train, either 'iwae', 'vae', or 'both'. Default = 'both'\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    model: keras model or tuple\n",
    "        If train is either 'iwae' or 'vae', returns a single trained keras model. Otherwise\n",
    "        returns a tuple of keras models, where model=(iwae_model, vae_model).\n",
    "        \n",
    "    hist: keras history or tuple\n",
    "        If train is either 'iwae' or 'vae', returns a single history object containing the\n",
    "        training history of the model. Otherwise returns a tuple of history objects.\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    \n",
    "    [1] Burda Y, Grosse R, Salakhutdinov R. Importance weighted autoencoders. \n",
    "        arXiv preprint arXiv:1509.00519. 2015 Sep 1.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    hists = []\n",
    "    \n",
    "    if train.lower() == 'iwae' or train.lower() == 'both':\n",
    "        # IWAE training\n",
    "        print('Training IWAE model')\n",
    "        print('-------------------')\n",
    "\n",
    "        iwae_model, z_mean, log_z_var, z1_samples, out_samples = model_1(k, latent_dim)\n",
    "        \n",
    "        def iwae_loss(y_true, y_pred):\n",
    "            # calculate log distributions (log_p_x_y := p(x|y))\n",
    "            log_weights = []\n",
    "            for i in range(k):\n",
    "                log_q_h1_x = -0.5 * K.sum(log_z_var + K.exp(-log_z_var)*(z1_samples[i] - z_mean)**2, axis=-1)\n",
    "                log_p_h1 = -0.5 * K.sum(K.square(z1_samples[i]), axis=-1)\n",
    "                log_p_x_h1 = -K.sum(K.binary_crossentropy(y_true, out_samples[i]), axis=(-1, -2))\n",
    "\n",
    "                # calculate weights\n",
    "                log_weight = log_p_x_h1 + log_p_h1 - log_q_h1_x\n",
    "                log_weights.append(log_weight)\n",
    "                \n",
    "            max_log_weights = K.max(log_weights, axis=0, keepdims=True) # for numerical stability\n",
    "            weights = K.exp(log_weights - max_log_weights)\n",
    "            elbo = 1/k * K.sum(weights, axis=0)\n",
    "            elbo = K.log(elbo) + max_log_weights\n",
    "            \n",
    "            # uncomment to add second order term to the elbo\n",
    "            #v_sqr = K.square(log_weights)\n",
    "            #v_sqr = 1/k * K.sum(v_sqr, axis=0)\n",
    "            #second_ord = -1/2 * (v_sqr - elbo**2)\n",
    "            #elbo = elbo + second_ord\n",
    "            \n",
    "            loss = -elbo\n",
    "\n",
    "            return loss\n",
    "        \n",
    "        iwae_model.compile(optimizer='adam', loss=iwae_loss)\n",
    "        hist = iwae_model.fit(x_train, x_train, batch_size=batch_size, epochs=epochs)\n",
    "        \n",
    "        out.append(iwae_model)\n",
    "        hists.append(hist)\n",
    "\n",
    "        model_path = './iwae_model_1_k_%d_dim_%d.weights' %(k, latent_dim)\n",
    "        iwae_model.save_weights(model_path)\n",
    "        \n",
    "        \n",
    "    if train.lower() == 'vae' or train.lower() == 'both':\n",
    "        # VAE training\n",
    "        if train.lower() == 'both':\n",
    "            print('\\n')\n",
    "        print('Training VAE model')\n",
    "        print('------------------')\n",
    "        \n",
    "        vae_model, z_mean, log_z_var, z1_samples, out_samples = model_1(k, latent_dim)\n",
    "        \n",
    "        def vae_loss(y_true, y_pred):\n",
    "            loss = 0\n",
    "            elbos = []\n",
    "            for i in range(k):\n",
    "                log_q_h1_x = -0.5 * K.sum(log_z_var + K.exp(-log_z_var)*(z1_samples[i] - z_mean)**2, axis=-1)\n",
    "                log_p_h1 = -0.5 * K.sum(K.square(z1_samples[i]), axis=-1)\n",
    "                log_p_x_h1 = -K.sum(K.binary_crossentropy(y_true, out_samples[i]), axis=(-1, -2))\n",
    "\n",
    "                elbo = log_p_x_h1 + log_p_h1 - log_q_h1_x\n",
    "                elbos.append(elbo)\n",
    "                loss -= elbo\n",
    "            \n",
    "            loss = loss / k\n",
    "            \n",
    "            # uncomment to see what happens when we add the second order term to the vae loss\n",
    "            #elbo = 1/k * K.sum(elbos, axis=0)\n",
    "            #v_sqr = K.square(elbos)\n",
    "            #v_sqr = 1/k * K.sum(v_sqr, axis=0)\n",
    "            #second_ord = -1/2 * (v_sqr - elbo**2)\n",
    "            #elbo = elbo + second_ord\n",
    "            \n",
    "            #loss = -elbo\n",
    "            \n",
    "            return loss\n",
    "\n",
    "        vae_model.compile(optimizer='adam', loss=vae_loss)\n",
    "        hist = vae_model.fit(x_train, x_train, batch_size=batch_size, epochs=epochs)\n",
    "        \n",
    "        out.append(vae_model)\n",
    "        hists.append(hist)\n",
    "\n",
    "        model_path = './vae_model_1_k_%d_dim_%d.weights' %(k, latent_dim)\n",
    "        vae_model.save_weights(model_path)\n",
    "    \n",
    "    \n",
    "    if train.lower() not in ['both', 'vae', 'iwae']:\n",
    "        print('Set parameter train to \"both\", \"vae\", or \"iwae\".')\n",
    "        return\n",
    "    \n",
    "    if train.lower() == 'both':\n",
    "        return out, hists\n",
    "    return out[0], hists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch trainer\n",
    "\n",
    "def train_for_ks(ks, latent_dim=100, train='both'):\n",
    "    \"\"\"Trains a model for each k in a list of ks.\"\"\"\n",
    "    ks = np.asarray(ks)\n",
    "    print('TRAINING')\n",
    "    for k in ks:\n",
    "        print('\\n')\n",
    "        print('-------------------')\n",
    "        print('k = %d' %k)\n",
    "        print('latent_dim = %d' %latent_dim)\n",
    "        print('-------------------\\n')\n",
    "        train_model_1(k, latent_dim, train=train)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ks = [1, 5, 10, 20, 30, 40, 50]\n",
    "ks = [1, 5, 10, 20, 30, 40, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to train all models (will take long!)\n",
    "#train_for_ks(ks, train='iwae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IWAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iwae model and get relevant predictions\n",
    "\n",
    "k = 100\n",
    "\n",
    "load_k = 50\n",
    "latent_dim = 100\n",
    "\n",
    "iwae_path = 'iwae_model_1_k_%d_dim_%d.weights' %(load_k, latent_dim)\n",
    "    \n",
    "# load model\n",
    "iwae_model, z_mean, log_z_var, z1_samples, out_samples = model_1(k, latent_dim, always_sample=True)\n",
    "iwae_model.load_weights(iwae_path, by_name=True)\n",
    "\n",
    "k_model = Model(iwae_model.input, [*out_samples, *z1_samples])\n",
    "enc_model = Model(iwae_model.input, [z_mean, log_z_var])\n",
    "\n",
    "#iwae_model.compile(optimizer='adam', loss=iwae_loss)\n",
    "\n",
    "k_outs = k_model.predict(x_test, batch_size=512)\n",
    "x_preds, z1_samples = k_outs[:k], k_outs[k:]\n",
    "z_mean, log_z_var = enc_model.predict(x_test, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iwae_elbo(y_true, y_pred):\n",
    "    # calculate log distributions (log_p_x_y := p(x|y))\n",
    "    elbo = []\n",
    "    log_weights = []\n",
    "    for i in range(k):\n",
    "        log_q_h1_x = -0.5 * np.sum(log_z_var + np.exp(-log_z_var)*(z1_samples[i] - z_mean)**2, axis=-1)\n",
    "        log_p_h1 = -0.5 * np.sum(z1_samples[i]**2, axis=-1)\n",
    "        log_p_x_h1 = np.sum(y_true * np.log(y_pred[i]) + (1 - y_true) * np.log(1 - y_pred[i]), axis=(-1, -2))\n",
    "\n",
    "        # calculate weights\n",
    "        log_weight = log_p_x_h1 + log_p_h1 - log_q_h1_x\n",
    "        log_weights.append(log_weight)\n",
    "    \n",
    "    weights = np.exp(log_weights - np.max(log_weights, axis=1, keepdims=True))\n",
    "    weights = weights / sum(weights, axis=0)\n",
    "    var = np.var(log_weights, axis=0, ddof=1)\n",
    "    elbo = np.sum(weights * log_weights, axis=0)\n",
    "    \n",
    "    #max_log_weights = np.max(log_weights, axis=0, keepdims=True)\n",
    "    #weights = np.exp(log_weights - max_log_weights)\n",
    "    #elbo = 1/k * np.sum(weights, axis=0)\n",
    "    #elbo = np.log(elbo) + max_log_weights\n",
    "    #loss = -elbo\n",
    "\n",
    "    return elbo, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo, var = iwae_elbo(x_test, x_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ -74.74732825, -117.35784069,  -49.16803788, ...,  -83.09872852,\n",
       "        -110.29914138, -134.28125874]),\n",
       " -98.95751206189958,\n",
       " array([ 47.08803697, 210.73754853,  32.39302402, ...,  33.69196127,\n",
       "         89.72099894, 156.11066039]),\n",
       " 95.85488116086499)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elbo, mean(elbo), var, mean(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of active units\n",
    "A_u = np.var(z_mean, axis=0)\n",
    "sum(log(A_u) >= -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iwae model and get relevant predictions\n",
    "\n",
    "k = 100\n",
    "\n",
    "load_k = 50\n",
    "latent_dim = 100\n",
    "\n",
    "vae_path = 'vae_model_1_k_%d_dim_%d.weights' %(load_k, latent_dim)\n",
    "    \n",
    "# load model\n",
    "vae_model, z_mean, log_z_var, z1_samples, out_samples = model_1(k, latent_dim, always_sample=True)\n",
    "vae_model.load_weights(vae_path, by_name=True)\n",
    "\n",
    "k_model = Model(vae_model.input, [*out_samples, *z1_samples])\n",
    "enc_model = Model(vae_model.input, [z_mean, log_z_var])\n",
    "\n",
    "#vae_model.compile(optimizer='adam', loss=vae_loss)\n",
    "\n",
    "k_outs = k_model.predict(x_test, batch_size=512)\n",
    "x_preds, z1_samples = k_outs[:k], k_outs[k:]\n",
    "z_mean, log_z_var = enc_model.predict(x_test, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_elbo(y_true, y_pred):\n",
    "    # calculate log distributions (log_p_x_y := p(x|y))\n",
    "    elbos = []\n",
    "    for i in range(k):\n",
    "        log_q_h1_x = -0.5 * np.sum(log_z_var + np.exp(-log_z_var)*(z1_samples[i] - z_mean)**2, axis=-1)\n",
    "        log_p_h1 = -0.5 * np.sum(z1_samples[i]**2, axis=-1)\n",
    "        log_p_x_h1 = np.sum(y_true * np.log(y_pred[i]) + (1 - y_true) * np.log(1 - y_pred[i]), axis=(-1, -2))\n",
    "        \n",
    "        elbos.append(log_p_x_h1 + log_p_h1 - log_q_h1_x)\n",
    "    \n",
    "    var = np.var(elbos, axis=0, ddof=1)\n",
    "    elbo = np.mean(elbos, axis=0)\n",
    "\n",
    "    return elbo, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo, var = vae_elbo(x_test, x_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ -78.95494078, -118.90137086,  -44.69199043, ...,  -88.82829875,\n",
       "        -115.88724153, -129.71096545]),\n",
       " -103.1800145100259,\n",
       " array([ 4.00670403,  9.42277608,  0.57595498, ..., 14.54460937,\n",
       "         9.90565738,  4.18842899]),\n",
       " 9.099651940002172)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elbo, mean(elbo), var, mean(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of active units\n",
    "A_u = np.var(z_mean, axis=0)\n",
    "sum(log(A_u) >= -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
